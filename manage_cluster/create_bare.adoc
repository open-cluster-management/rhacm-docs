[#creating-a-cluster-on-bare-metal]
= Creating a cluster on bare metal

You can use the {product-title} console to create a {ocp} cluster in a bare metal environment.

[#bare_prerequisites]
== Prerequisites

You need the following prerequisites before creating a cluster in a bare metal environment:

* A deployed {product-title} hub cluster on {ocp-short} version 4.5, or later.
* Internet access for your {product-title} hub cluster (connected) or a connection to an internal or mirror registry that has a connection to the Internet (disconnected) to retrieve the required images for creating the cluster.
* Bare metal provider connection;
see xref:../manage_cluster/prov_conn_bare.adoc#creating-a-provider-connection-for-bare-metal[Creating a provider connection for bare metal] for more information
* Login credentials for your bare metal environment, which include user name, password, and Baseboard Management Controller Address
* A {ocp} image pull secret;
see https://docs.openshift.com/container-platform/4.4/openshift_images/managing_images/using-image-pull-secrets.html[Using image pull secrets]
+
*Note:* If you change your cloud provider access key, you must manually update the provisioned cluster access key. For more information, see the known issue, link:../release_notes/known_issues.adoc#automatic-secret-updates-for-provisioned-clusters-is-not-supported[Automatic secret updates for provisioned clusters is not supported].


[#bare_creating-your-cluster-with-the-red-hat-advanced-cluster-management-for-kubernetes-console]
== Creating your cluster with the {product-title-short} console

To create clusters from the {product-title-short} console, complete the following steps:

. From the navigation menu, navigate to *Automate infrastructure* > *Clusters*.
. On the Clusters page, Click *Add Cluster*.
. Select *Create a cluster*.
+
*Note:* This procedure is for creating a cluster.
If you have an existing cluster that you want to import, see xref:../manage_cluster/import.adoc#importing-a-target-managed-cluster-to-the-hub-cluster[Importing a target managed cluster to the hub cluster] for those steps.

. Enter a name for your cluster. For a bare metal cluster, this name cannot be an arbitrary name. It is associated with the cluster URL. Make sure that the cluster name that you use is consistent with your DNS and network setup.
+
*Tip:* You can view the `yaml` content updates as you enter the information in the console by setting the _YAML_ switch to *ON*.

. Select *Bare Metal* for the infrastructure platform.
. Specify a *Release image* that you want to use for the cluster.
This identifies the version of the {ocp} image that is used to create the cluster.
If the version that you want to use is available, you can select the image from the list of images.
If the image that you want to use is not a standard image, you can enter the url to the image that you want to use.
See xref:../manage_cluster/release_images.adoc#release-images[Release images] for more information about release images.
. Select your provider connection from the available connections on the list.
If you do not have one configured, or want to configure a new one, select *Add provider*. See xref:../manage_cluster/prov_conn_bare.adoc#creating-a-provider-connection-for-bare-metal[Creating a provider connection for bare metal] for more information about creating a provider connection.
. Enter the base domain information that you configured in your bare metal environment. If there is already a base domain associated with the selected provider connection, that value is populated in that field. You can change the value by overwriting it. For a bare metal cluster, this setting is associated with the cluster URL. Make sure that the base domain that you use is consistent with your DNS and network setup.
. Select your hosts from the list of hosts that are associated with your provider connection.
Select a minimum of three assets that are on the same bridge networks as the hypervisor.
. Configure the cluster networking options.
+
|===
| Parameter | Description | Required or Optional

| Base DNS domain | The base domain of your provider, which is used to create routes to your {ocp} cluster components. It is configured in your cluster provider's DNS as a Start of Authority (SOA) record. This setting cannot be changed after the cluster is created. | Required
| Network type | The pod network provider plug-in to deploy. Only the OpenShiftSDN plug-in is supported on {ocp-short} 4.3. The OVNKubernetes plug-in is available as a technical preview on {ocp-short} 4.3. The default value is `OVNKubernetes`. | Required
| Cluster network CIDR | A block of IP addresses from which pod IP addresses are allocated. The OpenShiftSDN network plug-in supports multiple cluster networks. The address blocks for multiple cluster networks must not overlap. Select address pools large enough to fit your anticipated workload. The default values is 10.128.0.0/14. | Required
| Network host prefix | The subnet prefix length to assign to each individual node. For example, if hostPrefix is set to 23, then each node is assigned a /23 subnet out of the given CIDR, allowing for 510 (2^(32-23)-2) pod IP addresses. The default is 23. | Required
| Service network CIDR | A block of IP addresses for services. OpenShiftSDN allows only one serviceNetwork block. The address must not overlap any other network block. The default value is 172.30.0.0/16. | Required
| Machine CIDR | A block of IP addresses used by the {ocp-short} hosts. The address block must not overlap any other network block. The default value is 10.0.0.0/16. | Required
| Provisioning network CIDR | The CIDR for the network to use for provisioning. The example format is: 172.30.0.0/16. | Required
| Provisioning network interface | The name of the network interface on the control plane nodes that are connected to the provisioning network. | Required
| Provisioning network bridge | The name of the bridge on the hypervisor that is attached to the provisioning network. | Required
| External network bridge | The name of the bridge of the hypervisor that is attached to the external network. | Required
| API VIP | The Virtual IP to use for internal API communication. The DNS must be pre-configured with an A/AAAA or CNAME record so the `api.<cluster_name>.<Base DNS domain>` path resolves correctly. | Required
| Ingress VIP | The Virtual IP to use for ingress traffic. The DNS must be pre-configured with an A/AAAA or CNAME record so the `*.apps.<cluster_name>.<Base DNS domain>` path resolves correctly. | Optional
|===


. *Optional:* Configure a label for the cluster.
. *Optional:* Update the advanced settings, if you want to change the setting for including a configmap.
. Click *Create*.
You can view your cluster details after the create and import process is complete.

+
*Note:* You do not have to run the `kubectl` command that is provided with the cluster details to import the cluster. When you create the cluster, it is automatically configured under the management of {product-title}. 

[#bare_accessing-your-cluster]
== Accessing your cluster

To access a cluster that is managed by {product-title}, complete the following steps:

. From the {product-title} navigation menu, navigate to *Automate infrastructure* > *Clusters*.
. Select the name of the cluster that you created or want to access.
The cluster details are displayed.
. Select *Reveal credentials* to view the user name and password for the cluster.
Note these values to use when you log in to the cluster.
. Select *Console URL* to link to the cluster.
. Log in to the cluster by using the user ID and password that you found in step 3.
. Select *Actions* > *Launch to cluster* for the cluster that you want to access.
+
*Tip:* If you already know the login credentials, you can access the cluster by selecting *Actions* > *Launch to cluster* for the cluster that you want to access.
