[#known-issues]
= Known issues

////
Please follow this format:

Title of known issue, be sure to match header and make title, header unique

Hidden comment: Release: #issue
Known issue with workaround if:

- Doesn't work the way it should
- Straightforward to describe
- Good to know before getting started
- Quick workaround, of any
- Applies to most, if not all, users
- Something that is likely to be fixed next release (never preannounce)

Or consider a troubleshooting topic.
////

Review the known issues for Red Hat Advanced Cluster Management for Kubernetes. The following list contains known issues for this release, or known issues that continued from the previous release.

* <<installation-known-issues,Installation known issues>>
* <<web-console-known-issues,Web console known issues>>
* <<cluster-management-issues,Cluster management known issues>>
* <<application-management-known-issues,Application management known issues>>
* <<security-known-issues,Security known issues>>

[#installation-known-issues]
== Installation known issues

[#host-adoption-failed]
=== Host adoption failed

// 1.0.0:1220

Bare metal hosts are not supported.
An error message appears when you try to validate the host adoption.

[#upgrading-an-ibm-red-hat-openshift-kubernetes-service-managed-cluster-is-not-supported]
=== Upgrading an IBM Red Hat OpenShift Kubernetes Service managed cluster is not supported

// 1.0.0:2131

You cannot upgrade an IBM Red Hat OpenShift Kubernetes Service managed cluster by using the Red Hat Advanced Cluster Management for Kubernetes interface.

[#importing-certain-versions-of-ibm-red-hat-openshift-kubernetes-service-clusters-is-not-supported]
=== Importing certain versions of IBM Red Hat OpenShift Kubernetes Service clusters is not supported

// 1.0.0:2179

You cannot import IBM Red Hat OpenShift Kubernetes Service version 3.11 clusters.
Later versions of IBM OpenShift Kubernetes Service are supported.

[#certificate-manager-must-not-exist-during-an-installation]
=== Certificate manager must not exist during an installation

// 1.0.0:678

Certificate manager must not exist on a cluster when you install Red Hat Advanced Cluster Management for Kubernetes.

When certificate manager already exists on the cluster, Red Hat Advanced Cluster Management for Kubernetes installation fails.

To resolve this issue, verify if the certificate manager is present in your cluster by running the following command:

----
   kubectl get crd | grep certificates.certmanager
----

[#all-users-with-access-to-the-red-hat-advanced-cluster-management-component-namespaces-are-automatically-granted-cluster-administrator-access]
=== All users with access to the Red Hat Advanced Cluster Management component namespaces are automatically granted cluster administrator access

// 1.0.0:2135

The Red Hat Advanced Cluster Management components are installed in a single namespace.
A `ServiceAccount` with a `ClusterRoleBinding` automatically gives cluster administrator privileges to Red Hat Advanced Cluster Management and to any ID with access to the namespace.
For security, do not give anyone access to this namespace who does not already have cluster administrator access.

The `multicluster-endpoint`, which is the agent on the managed cluster, also requires cluster administrator privileges.
The `multicluster-endpoint` is deployed into the `multicluster-endpoint` namespace.
For security, do not give anyone access to the `multicluster-endpoint` namespace who does not already have cluster administrator access.

[#the-etcd-operator-might-not-install-correctly]
=== The etcd-operator might not install correctly

// 1.0.1:no issue

When installing Red Hat Advanced Cluster Management on Red Hat OpenShift Container Platform version 4.4.8, or later, the `etcd-operator` might not install in the correct default channel.
When this happens, the `etcd-operator` pods do not start.

These solutions only work when the `etcdcluster` subscription is installed in `clusterwide` mode.
You can determine if it is in `clusterwide` mode when the `etcdcluster` subscription is using the `clusterwide-alpha` channel.

To work around this issue, complete one of the following procedures:

* Run the following command to add the required annotation to the cluster resource:
+
----
 oc annotate etcdcluster etcd-cluster etcd.database.coreos.com/scope=clusterwide
----

* Add the following annotations to the cluster resource:
+
----
 apiVersion: "etcd.database.coreos.com/v1beta2"
 kind: "EtcdCluster"
 metadata:
   name: "example-etcd-cluster"
   ## Adding this annotation make this cluster managed by clusterwide operators
   ## namespaced operators ignore it
   # annotations:
   #   etcd.database.coreos.com/scope: clusterwide
 spec:
   size: 3
   version: "3.2.13"
----

[#web-console-known-issues]
== Web console known issues

[#ldap-user-names-are-case-sensitive]
=== LDAP user names are case-sensitive

// 1.0.0:before 1.0.0.1

LDAP user names are case-sensitive.
You must use the name exactly the way it is configured in your LDAP directory.

[#console-features-might-not-display-in-firefox-earlier-versions]
=== Console features might not display in Firefox earlier versions

// 1.0.0:before 1.0.0.1

The product supports Mozilla Firefox 74.0 or the latest version that is available for Linux, macOS, and Windows.
Upgrade to the latest version for the best console compatibility.

[#unable-to-search-using-values-with-empty-spaces]
=== Unable to search using values with empty spaces

// 1.0.0:1726

From the console and Visual Web Terminal, users are unable to search for values that contain an empty space.

[#a-saved-search-in-the-console-might-display-incorrect-value-in-visual-web-terminal]
=== A saved search in the console might display incorrect value in Visual Web Terminal

// 1.0.0:1726

If you save your search query and use `savedsearches`, you might see incorrect results in the Visual Web Terminal.
The result from the _Search_ page is correct.

[#at-logout-user-kubeadmin-gets-extra-browser-tab-with-blank-page]
=== At logout user kubeadmin gets extra browser tab with blank page

// 1.0.0:2191

When you are logged in as `kubeadmin` and you click the *Log out* option in the drop-down menu, the console returns to the login screen, but a browser tab opens with a `/logout` URL.
The page is blank and you can close the tab without impact to your console.

[#cluster-management-issues]
== Cluster management known issues

[#unsupported-value-when-generating-a-new-import-cluster-command-causes-imports-to-fail]
=== Unsupported value when generating a new import cluster command causes imports to fail

// 1.0.0:2747

When you enter an unsupported value into the `yaml` content before creating your import command, it causes the import command that you are creating and future import commands to fail with the following error:

----
   Failed creating cluster resource for import,:secrets "<your_cluster_name>-import" not found.
----

You can fix this issue by identifying which `endpointconfig` is causing the problem and manually changing the unsupported value to a supported value.

. Check the rcm-controller's log to try to determine which `endpointconfig` is causing the problem.
It is often something like `"yes"` used as a value when `true` is the supported value.
An example of the log entry follows:
+
----
E0611 19:28:03.137671       1 reflector.go:123] pkg/mod/k8s.io/client-go@v0.0.0-20191016111102-bec269661e48/tools/cache/reflector.go:96: Failed to list *v1alpha1.EndpointConfig: v1alpha1.EndpointConfigList.Items: []v1alpha1.EndpointConfig: v1alpha1.EndpointConfig.Spec: v1beta1.EndpointSpec.CISControllerConfig: v1beta1.EndpointCISControllerSpec.Enabled: ReadBool: expect t or f, but found ", error found in #10 byte of ...|enabled":"yes"},"clu|..., bigger context ...|ler":{"enabled":true},"cisController":{"enabled":"yes"},"clusterLabels":{"cloud":"auto-detect","vend|...
----
+
In this example, the `cisController` value should be `true`, but is `"yes"`.

. Determine which `endpointconfig` contains the issue.
by entering the following command:
+
----
oc get endpointconfig --all-namespaces -o yaml | grep -B40 "yes"  | grep 'name: ' | tail -n1
----
+
This command finds the name of the `endpointconfig` that contains the value of `"yes"`.
+
A result that is similar to the following content is displayed:
+
----
name: mycluster1
----

. Replace the value `"yes"` with the value `true` by editing the file with a command similar to the following:
+
----
oc edit -n mycluster1 mycluster1
----

*Notice:* If you have this problem in one of your imported clusters, you cannot import another cluster successfully until you fix it.
Even if you import a different cluster, the import fails until the problem is fixed.

[#application-management-known-issues]
== Application management known issues

[#edit-role-for-application-error]
=== Edit role for application error

// 2.0.0:1681

A user performing in an `Editor` role should only have `read` or `update` authority on an application, but erroneously editor can also `create` and `delete` an application. Red Hat Open Shift Operator Lifecycle Manager default settings change the setting for the product. To workaround the issue, see the following procedure:

1. Run `oc edit clusterrole applications.app.k8s.io-v1beta1-edit -o yaml` to open the application edit cluster role.
2. Remove `create` and `delete` from the verbs list.
3. Save the change.

[#application-not-deployed-after-an-updated-placement-rule]
=== Application not deployed after an updated placement rule

// 1.0.0:1449

If applications are not deploying after an update to a placement rule, verify that the `klusterlet-addon-appmgr` pod is running.
The `klusterlet-addon-appmgr` is the subscription container that needs to run on endpoint clusters.

You can run `oc get pods -n open-cluster-management-agent-addon ` to verify.

You can also search for `kind:pod cluster:yourcluster` in the console and see if the `klusterlet-addon-appmgr` is running.

If you cannot verify, attempt to import the cluster again and verify again.

[#subscription-operator-does-not-create-an-scc]
=== Subscription operator does not create an SCC

// 1.0.0:1764

Learn about Red Hat Openshift Container Platform SCC at https://docs.openshift.com/container-platform/4.3/authentication/managing-security-context-constraints.html#security-context-constraints-about_configuring-internal-oauth[Managing Security Context Constraints (SCC)], which is an additional configuration required on the managed cluster.

Different deployments have different security context and different service accounts.
The subscription operator cannot create an SCC automatically.
Administrators control permissions for pods.
A Security Context Constraints (SCC) CR is required to enable appropriate permissions for the relative service accounts to create pods in the non-default namespace:

To manually create an SCC CR in your namespace, complete the following:

. Find the service account that is defined in the deployments.
For example, see the following `nginx` deployments:
+
----
 nginx-ingress-52edb
 nginx-ingress-52edb-backend
----

. Create an SCC CR in your namespace to assign the required permissions to the service account or accounts.
See the following example where `kind: SecurityContextConstraints` is added:
+
----
 apiVersion: security.openshift.io/v1
 defaultAddCapabilities:
 kind: SecurityContextConstraints
 metadata:
   name: ingress-nginx
   namespace: ns-sub-1
 priority: null
 readOnlyRootFilesystem: false
 requiredDropCapabilities:
 fsGroup:
   type: RunAsAny
 runAsUser:
   type: RunAsAny
 seLinuxContext:
   type: RunAsAny
 users:
 - system:serviceaccount:my-operator:nginx-ingress-52edb
 - system:serviceaccount:my-operator:nginx-ingress-52edb-backend
----

[#application-channels-require-unique-namespaces]
=== Application channels require unique namespaces

// 1.0.0:2311

Creating more than one channel in the same namespace can cause errors with the hub cluster.

For instance, namespace `charts-v1` is used by the installer as a Helm type channel, so do not create any additional channels in `charts-v1`.
Ensure that you create your channel in a unique namespace.

For product version 1.0, all channels need an individual namespace, except GitHub channels, which can share a namespace with another GitHub channel.
See the process for link:../manage_applications/managing_channels.adoc[Creating and managing channels] for more information.

[#application-route-does-not-list-in-the-search-page-for-cluster]
=== Application route does not list in the Search page for cluster

// 1.0.0:1908

If none of the targeted managed clusters for a deployed application is a Red Hat OpenShift cluster, the Route resource is not created, even if the _Application Topology_ displays the object.
Since the object is not deployed, it does not display during a Search.

This is the case for all Kubernetes resources that are platform-specific, such as the Route resource.
The Application defines these resources, but they are created on the managed clusters only if they match the resource platform.

[#security-known-issues]
== Security known issues

[#certificate-policies-fail-to-report-status]
=== Certificate policies fail to report status

// 1.0.0:2302

You can create and apply multiple certificate policies on a single managed cluster, but each policy must have a different parameter value for the `namespaceSelector`.
When mulitiple policies on the same managed cluster use the same `namespaceSelector` value, only one of the policies work as expected.

For more information, see link:../security/cert_policy_ctrl.adoc[Certificate policy controller].

[#internal-error-500-during-login-to-the-console]
== Internal error 500 during login to the console

// 1.0.1:2414

When Red Hat Advanced Cluster Management for Kubernetes is installed and the OpenShift Container Platform is customized with a custom ingress certificate, a `500 Internal Error` message appears.
You are unable to access the console because the OpenShift Container Platform certificate is not included in the Red Hat Advanced Cluster Management for Kuberentes management ingress.
Add the OpenShift Container Platform certificate by completing the following steps:

. Create a ConfigMap that includes the certificate authority used to sign the new certificate. Your ConfigMap must be identical to the one you created in the `openshift-config` namespace. Run the following command:

+
----
oc create configmap custom-ca \
     --from-file=ca-bundle.crt=</path/to/example-ca.crt> \
     -n open-cluster-management
----

. Edit your `multiclusterhub` YAML file by running the following command:

+
----
oc edit multiclusterhub multiclusterhub
----

.. Update the `spec` section by editing the parameter value for `customCAConfigmap`. The parameter might resemble the following content:

+
----
customCAConfigmap: custom-ca
----

After you complete the steps, wait a few minutes for the changes to propagate to the charts and log in again. The OpenShift Container Platform certificate is added.
