[#known-issues]
= Known issues

////
Please follow this format:

Title of known issue, be sure to match header and make title, header unique

Hidden comment: Release: #issue
Known issue with workaround if:

- Doesn't work the way it should
- Straightforward to describe
- Good to know before getting started
- Quick workaround, of any
- Applies to most, if not all, users
- Something that is likely to be fixed next release (never preannounce)

Or consider a troubleshooting topic.
////

Review the known issues for Red Hat Advanced Cluster Management for Kubernetes. The following list contains known issues for this release, or known issues that continued from the previous release.

* <<installation-known-issues,Installation known issues>>
* <<web-console-known-issues,Web console known issues>>
* <<cluster-management-issues,Cluster management known issues>>
* <<application-management-known-issues,Application management known issues>>
* <<security-known-issues,Security known issues>>

[#installation-known-issues]
== Installation known issues

[#openshift-container-platform-cluster-upgrade-failed-status]
=== Openshift Container Platform cluster upgrade failed status

// 2.0.0:3442

When an Openshift Container Platform cluster is in the upgrade stage, the cluster pods are restarted and the cluster might remain in `upgrade failed` status for a variation of 1-5 minutes. This behavior is expected and resolves after a few minutes.

[#certificate-manager-must-not-exist-during-an-installation]
=== Certificate manager must not exist during an installation

// 1.0.0:678

Certificate manager must not exist on a cluster when you install Red Hat Advanced Cluster Management for Kubernetes.

When certificate manager already exists on the cluster, Red Hat Advanced Cluster Management for Kubernetes installation fails.

To resolve this issue, verify if the certificate manager is present in your cluster by running the following command:

----
kubectl get crd | grep certificates.certmanager
----

[#web-console-known-issues]
== Web console known issues

[#ldap-user-names-are-case-sensitive]
=== LDAP user names are case-sensitive

// 1.0.0:before 1.0.0.1

LDAP user names are case-sensitive.
You must use the name exactly the way it is configured in your LDAP directory.

[#console-features-might-not-display-in-firefox-earlier-versions]
=== Console features might not display in Firefox earlier versions

// 1.0.0:before 1.0.0.1

The product supports Mozilla Firefox 74.0 or the latest version that is available for Linux, macOS, and Windows.
Upgrade to the latest version for the best console compatibility.

[#unable-to-search-using-values-with-empty-spaces]
=== Unable to search using values with empty spaces

// 1.0.0:1726

From the console and Visual Web Terminal, users are unable to search for values that contain an empty space.

[#at-logout-user-kubeadmin-gets-extra-browser-tab-with-blank-page]
=== At logout user kubeadmin gets extra browser tab with blank page

// 1.0.0:2191

When you are logged in as `kubeadmin` and you click the *Log out* option in the drop-down menu, the console returns to the login screen, but a browser tab opens with a `/logout` URL.
The page is blank and you can close the tab without impact to your console.

[#cluster-management-issues]
== Cluster management known issues

[#importing-clusters-might-require-two-attempts]
=== Importing clusters might require two attempts

// 2.0.0:3596

When you import a cluster that was previously managed and detached by a {product-title-short} hub cluster, the import process might fail the first time. The cluster status is `pending import`. Run the command again, and the import should be successful. 

[#klusterlet-runs-on-a-detached-cluster]
=== Klusterlet runs on a detached cluster

// 2.0.0:3460

If you detach an online cluster immediately after it was attached, the Klusterlet starts to run on the detached cluster before the `manifestwork` syncs. Removal of the managed cluster from the hub cluster does not uninstall the Klusterlet. Complete the following steps to fix the issue:

. Download the link:https://github.com/open-cluster-management/deploy/blob/master/hack/cleanup-managed-cluster.sh[`cleanup-managed-cluster`] script from the `deploy` Git repository.

. Run the `cleanup-managed-cluster.sh` script by entering the following command:

+
----
./cleanup-managed-cluster.sh
----

[#importing-certain-versions-of-ibm-red-hat-openshift-kubernetes-service-clusters-is-not-supported]
=== Importing certain versions of IBM Red Hat OpenShift Kubernetes Service clusters is not supported

// 1.0.0:2179

You cannot import IBM Red Hat OpenShift Kubernetes Service version 3.11 clusters.
Later versions of IBM OpenShift Kubernetes Service are supported.

[#detaching-openshift-container-platform-3.11-does-not-remove-the-open-cluster-manangement-agent]
=== Detaching OpenShift Container Platform 3.11 does not remove the _open-cluster-manangement-agent_

// 2.0.0:3847

When you detach managed clusters on OpenShift Container Platform 3.11, the `open-cluster-management-agent` namesapce is not automatically deleted. Manually remove the namespace by running the following command:

----
oc delete ns open-cluster-management-agent
----

[#automatic-secret-updates-for-provisioned-clusters-is-not-supported]
=== Automatic secret updates for provisioned clusters is not supported

// 2.0.0:3702

When you change your cloud provider access key, the provisioned cluster access key is not updated in the namespace. Run the following command for your cloud provider to update the access key: 

* Amazon Web Services (AWS)

+
----
oc patch secret {CLUSTER-NAME}-aws-creds -n {CLUSTER-NAME} --type json -p='[{"op": "add", "path": "/stringData", "value":{"aws_access_key_id": "{YOUR-NEW-ACCESS-KEY-ID}","aws_secret_access_key":"{YOUR-NEW-aws_secret_access_key}"} }]'
----

* Google Cloud Platform (GCP)

+
----
oc set data secret/{CLUSTER-NAME}-gcp-creds -n {CLUSTER-NAME} --from-file=osServiceAccount.json=$HOME/.gcp/osServiceAccount.json
----

* Microsoft Azure 

+
----
oc set data secret/{CLUSTER-NAME}-azure-creds -n {CLUSTER-NAME} --from-file=osServiceAccount.json=$HOME/.azure/osServiceAccount.json
----

[#resources-remain-after-you-detach-an-offline-managed-cluster]
=== Resources remain after you detach an offline managed cluster

// 2.0:3210

When you detach a managed cluster that is in an offline state, there are some resources that cannot be removed from managed cluster. Complete the following steps to remove the additional resources:

. Make sure you have the `oc` command line interface configured.
. Make sure you have `KUBECONFIG` configured on your managed cluster.
+ 
If you run `oc get ns | grep open-cluster-management-agent` you should see two namespaces:
+
----
open-cluster-management-agent         Active   10m
open-cluster-management-agent-addon   Active   10m
----

. Download the link:https://github.com/open-cluster-management/deploy/blob/master/hack/cleanup-managed-cluster.sh[`cleanup-managed-cluster`] script from the `deploy` Git repository.
. Run the `cleanup-managed-cluster.sh` script by entering the following command:
+
----
./cleanup-managed-cluster.sh
----
. Run the following command to ensure that both namespaces are removed: 
+
----
oc get ns | grep open-cluster-management-agent 
----

[#no-run-mgt-ingress-nonroot]
=== Cannot run `management ingress` as non-root user

You must be logged in as `root` to run the `management-ingress` service. 

// 2.0:3532

[#self-destruct-script-does-not-clean-target-clusters]
=== Self-destruct script does not clean target clusters 

In some cases, a component operator crashes. When the component operator crashes, the self-destruct script does not remove all resources from target clusters. Complete the following steps to clean your target clusters:

. Remove your API resources by running the following command:

+
----
kubectl api-resources -o name --namespaced=true); do echo "===$api==="; kubectl get $api -n multicluster-endpoint
----

. Get a list of your subscriptions and delete them. Run the following command:

+
----
kubectl get subscriptions.apps.open-cluster-management.io

kubectl delete subscriptions.apps.open-cluster-management.io
----

. Get a list of your CustomResourceDefinitions (CRDs). Delete the application CRDs by running the following command:

+
----
kubectl get crd |grep apps.open-cluster-management.io

kubectl delete apps.open-cluster-management.io
----

. Run the following commands to delete the cluster role for the `endpoint-appmgr`:

+
----
kubectl get clusterrolebinding | grep ^endpoint | awk '{print $1}'

kubectl delete clusterrole endpoint-appmgr
----

. Run the following commands to delete the cluster rolebinding for the `endpoint-appmgr`:

+
----
kubectl get clusterrolebinding | grep ^endpoint | awk '{print $1}'

kubectl delete clusterrolebinding endpoint-appmgr
----

. What command can the user run to verify that the target cluster is clean? 


Your target cluster is cleaned. 

[#application-management-known-issues]
== Application management known issues

[#namespace-channel-subscription-remains-in-failed-state]
=== Namespace channel subscription remains in failed state
// 2.0.0:3581

When you subscribe to a namespace channel and the subscription remains in `FAILED` state after you fixed other associated resources such as channel, secret, configmap, or placement rule, the namespace subscription is not continuously reconciled. 

To force the subscription reconcile again to get out of `FAILED` state, complete the following steps:

. Log in to your hub cluster.
. Manually add a label to the subscription using the following command:

----
oc label subscriptions.apps.open-cluster-management.io the_subscription_name reconcile=true
----

[#deployable-resources-in-a-namespace-channel]
=== Deployable resources in a namespace channel

// 2.0.0:3435

You need to manually create deployable resources within the channel namespace. 

To create deployable resources correctly, add the following two labels that are required in the deployable to the subscription controller that identifies which deployable resources are added:

----
labels:
    apps.open-cluster-management.io/channel: <channel name>
    apps.open-cluster-management.io/channel-type: Namespace
----

Don't specify template namespace in each deployable `spec.template.metadata.namespace`. 

For the namespace type channel and subscription, all the deployable templates are deployed to the subscription namespace on managed clusters. As a result, those deployable templates that are defined outside of the subscription namespace are skipped.

See link:../manage_applications/managing_channels.adoc#creating-and-managing-channels[Creating and managing channels] for more information.

[#edit-role-for-application-error]
=== Edit role for application error

// 2.0.0:1681

A user performing in an `Editor` role should only have `read` or `update` authority on an application, but erroneously editor can also `create` and `delete` an application. Red Hat OpenShift Operator Lifecycle Manager default settings change the setting for the product. To workaround the issue, see the following procedure:

1. Run `oc edit clusterrole applications.app.k8s.io-v1beta1-edit -o yaml` to open the application edit cluster role.
2. Remove `create` and `delete` from the verbs list.
3. Save the change.

[#edit-role-for-placement-rule-error]
=== Edit role for placement rule error

// 2.0.0:3693

A user performing in an `Editor` role should only have `read` or `update` authority on an placement rule, but erroneously editor can also `create` and `delete`, as well. Red Hat OpenShift Operator Lifecycle Manager default settings change the setting for the product. To workaround the issue, see the following procedure:

1. Run `oc edit clusterrole placementrules.apps.open-cluster-management.io-v1-edit` to open the application edit cluster role.
2. Remove `create` and `delete` from the verbs list.
3. Save the change.

[#application-not-deployed-after-an-updated-placement-rule]
=== Application not deployed after an updated placement rule

// 1.0.0:1449

If applications are not deploying after an update to a placement rule, verify that the `klusterlet-addon-appmgr` pod is running.
The `klusterlet-addon-appmgr` is the subscription container that needs to run on endpoint clusters.

You can run `oc get pods -n open-cluster-management-agent-addon ` to verify.

You can also search for `kind:pod cluster:yourcluster` in the console and see if the `klusterlet-addon-appmgr` is running.

If you cannot verify, attempt to import the cluster again and verify again.

[#subscription-operator-does-not-create-an-scc]
=== Subscription operator does not create an SCC

// 1.0.0:1764

Learn about Red Hat Openshift Container Platform SCC at https://docs.openshift.com/container-platform/4.3/authentication/managing-security-context-constraints.html#security-context-constraints-about_configuring-internal-oauth[Managing Security Context Constraints (SCC)], which is an additional configuration required on the managed cluster.

Different deployments have different security context and different service accounts.
The subscription operator cannot create an SCC automatically.
Administrators control permissions for pods.
A Security Context Constraints (SCC) CR is required to enable appropriate permissions for the relative service accounts to create pods in the non-default namespace:

To manually create an SCC CR in your namespace, complete the following:

. Find the service account that is defined in the deployments.
For example, see the following `nginx` deployments:
+
----
 nginx-ingress-52edb
 nginx-ingress-52edb-backend
----

. Create an SCC CR in your namespace to assign the required permissions to the service account or accounts.
See the following example where `kind: SecurityContextConstraints` is added:
+
----
 apiVersion: security.openshift.io/v1
 defaultAddCapabilities:
 kind: SecurityContextConstraints
 metadata:
   name: ingress-nginx
   namespace: ns-sub-1
 priority: null
 readOnlyRootFilesystem: false
 requiredDropCapabilities:
 fsGroup:
   type: RunAsAny
 runAsUser:
   type: RunAsAny
 seLinuxContext:
   type: RunAsAny
 users:
 - system:serviceaccount:my-operator:nginx-ingress-52edb
 - system:serviceaccount:my-operator:nginx-ingress-52edb-backend
----

[#application-channels-require-unique-namespaces]
=== Application channels require unique namespaces

// 1.0.0:2311

Creating more than one channel in the same namespace can cause errors with the hub cluster.

For instance, namespace `charts-v1` is used by the installer as a Helm type channel, so do not create any additional channels in `charts-v1`.
Ensure that you create your channel in a unique namespace.

For product version 1.0, all channels need an individual namespace, except GitHub channels, which can share a namespace with another GitHub channel.
See the process for link:../manage_applications#creating-and-managing-channels[Creating and managing channels] for more information.

[#security-known-issues]
== Security known issues

[#internal-error-500-during-login-to-the-console]
== Internal error 500 during login to the console

// 1.0.1:2414

When Red Hat Advanced Cluster Management for Kubernetes is installed and the OpenShift Container Platform is customized with a custom ingress certificate, a `500 Internal Error` message appears.
You are unable to access the console because the OpenShift Container Platform certificate is not included in the Red Hat Advanced Cluster Management for Kuberentes management ingress.
Add the OpenShift Container Platform certificate by completing the following steps:

. Create a ConfigMap that includes the certificate authority used to sign the new certificate. Your ConfigMap must be identical to the one you created in the `openshift-config` namespace. Run the following command:

+
----
oc create configmap custom-ca \
     --from-file=ca-bundle.crt=</path/to/example-ca.crt> \
     -n open-cluster-management
----

. Edit your `multiclusterhub` YAML file by running the following command:

+
----
oc edit multiclusterhub multiclusterhub
----

.. Update the `spec` section by editing the parameter value for `customCAConfigmap`. The parameter might resemble the following content:

+
----
customCAConfigmap: custom-ca
----

After you complete the steps, wait a few minutes for the changes to propagate to the charts and log in again. The OpenShift Container Platform certificate is added.
