[#known-issues]
= Known issues

////
Please follow this format:

Title of known issue, be sure to match header and make title, header unique

Hidden comment: Release: #issue
Known issue with workaround if:

- Doesn't work the way it should
- Straightforward to describe
- Good to know before getting started
- Quick workaround, of any
- Applies to most, if not all, users
- Something that is likely to be fixed next release (never preannounce)

Or consider a troubleshooting topic.
////

Review the known issues for {product-title}. The following list contains known issues for this release, or known issues that continued from the previous release. For your {ocp} cluster, see link:https://docs.openshift.com/container-platform/4.3/release_notes/ocp-4-3-release-notes.html#ocp-4-3-known-issues[{ocp-short} known issues].

* <<upgrade-known-issues,Upgrade known issues>>
* <<installation-known-issues,Installation known issues>>
* <<web-console-known-issues,Web console known issues>>
* <<cluster-management-issues,Cluster management known issues>>
* <<application-management-known-issues,Application management known issues>>
* <<security-known-issues,Security known issues>>

[#upgrade-known-issues]
== Upgrade known issues

[#upgrade-cluster-drops-app-deploy-set]
=== Upgrade to 2.1.x results in loss of certificates

// 2.1.6:11591

After upgrading {product-title-short} from version 2.0 to version 2.1, the setting that specifies where an application is deployed is not pre-selected when you open the application template editor to make changes. If you make changes to the application settings in the application template editor, you must select application deployment setting before saving and closing the editor. 

[#upgrade-cluster-drops-certificates]
=== Upgrade to 2.1.1 results in loss of certificates

// 2.1.1:7533

When your cluster upgrades to {product-title-short} version 2.1.1, you lose some or all of the certificates on your cluster. You can confirm this situation by entering one of the following commands:

---- 
oc get certificates -n open-cluster-management
----

or

----
oc get pods -n open-cluster-management | grep -vE "Completed|Running"
----

If there are fewer certificates returned than expected when you run the first command, or more than one pod is returned after running the second command, run the link:https://gist.github.com/cdoan1/79451b3d75a2f3f1b74b3622029d6989[
 `generate-update-issue-cert-manifest.sh` script] to update the certificates. 

[#upgrade-does-not-complete-clusterset-error]
=== Upgrade to version 2.1.1 does not complete with ClusterImageSet error

// 2.1.1:7527

In some cases, your upgrade of {product-title} version 2.1.0 to {product-title-short} version 2.1.1 does not complete and displays an error that is similar to the following error:

----
failed to get candidate release: rendered manifests contain a resource
that already exists. Unable to continue with update: ClusterImageSet "img4.6.1-x86-64"
in namespace "" exists and cannot be imported into the current release: invalid
ownership metadata; label validation error: missing key "app.kubernetes.io/managed-by":
must be set to "Helm"; annotation validation error: missing key "meta.helm.sh/release-name":
must be set to "console-chart-c4cb5"; annotation validation error: missing key
"meta.helm.sh/release-namespace": must be set to "open-cluster-management"
----

This occurs when one or more ClusterImageSets on the existing version have the same names as the versions that are added with the upgrade, which causes a collision. To work around this issue, complete the following steps: 

. Stop the running upgrade.

. Delete the ClusterImageSet or ClusterImageSets from your local environment that are identified in the error message.

. Restart the upgrade.

[#upgrade-disables-klusterletaddonconfig]
=== Upgrade to 2.1.1 disables klusterletaddonconfig CRDs

// 2.1.1:7582

When your {product-title-short} upgrades from version 2.1.0 to version 2.1.1, the `klusterletaddonconfig` custom resource definitions (CRDs) might be reinstalled during the upgrade. If this happens, all of the add ons show a `Disabled` status on the _Cluster settings_ pages. Complete the following steps to diagnose the problem and restore the klusterletaddonconfig CRDs:

. Log on to the hub cluster by using the `oc login` command.

. Run the following command to confirm that the deleted `klusterletaddonconfig` CRDs occurred because of the reinstalled CRDs:
+
----
% oc get klusterletaddonconfig --all-namespaces 
----
+
If the returned content is `No resources found`, the reinstallation is likely the issue. Continue with step 3.

. Save the following script to a file. For this example, the filename is `restore-addons.sh`:
+
----
KUBECTL=oc
ACM_NAMESPACE=open-cluster-management

ACM_VERSION=$(${KUBECTL} get -n ${ACM_NAMESPACE} `${KUBECTL} get mch -oname -n ${ACM_NAMESPACE} | head -n1` -ojsonpath='{.status.desiredVersion}')
if [ "${ACM_VERSION}" = ""  ]; then
ACM_VERSION=2.1.1
fi

echo "ACM version: ${ACM_VERSION}"

for clusterName in `${KUBECTL} get managedcluster --ignore-not-found | grep -v "NAME" | awk '{ print $1 }'`; do
    echo "Checking klusterletaddonconfig in ${clusterName} namespace."
    ${KUBECTL} get klusterletaddonconfig ${clusterName} -n ${clusterName} >/dev/null 2>&1
    if [ "$?" != "0"  ]; then
        echo "  klusterletaddonconfig in ${clusterName} is missing."
        echo "  Creating..."
        printf "  "
        cat <<EOF | ${KUBECTL}  apply -f - 
apiVersion: agent.open-cluster-management.io/v1
kind: KlusterletAddonConfig
metadata:
  name: ${clusterName}
  namespace: ${clusterName}
spec:
  clusterLabels:
    cloud: auto-detect
    vendor: auto-detect
  clusterName: ${clusterName}
  clusterNamespace: ${clusterName}
  applicationManager:
    enabled: true
  certPolicyController:
    enabled: true
  iamPolicyController:
    enabled: true
  policyController:
    enabled: true
  searchCollector:
    enabled: true
  version: ${ACM_VERSION}
EOF

    fi
    echo "  Done."
done
----
+
Replace the value for `ACM_NAMESPACE` with the name of your namespace, if you did not install {product-title-short} in the `open-cluster-management` namespace.

. Run the script from CLI. Your command should resemble the following command:
+
----
chmod +x restore-addons.sh && ./restore-addons.sh
----
+
Running the script recreates the missing `klusterletaddonconfig` CRDs in each managed cluster namespace.

[#openshift-container-platform-cluster-upgrade-failed-status]
=== OpenShift Container Platform cluster upgrade failed status

// 2.0.0:3442

When an {ocp-short} cluster is in the upgrade stage, the cluster pods are restarted and the cluster might remain in `upgrade failed` status for a variation of 1-5 minutes. This behavior is expected and resolves after a few minutes.

[#upgrade-leaves-clusterserviceversion-pending]
=== Upgrade from version 2.0.4 to version 2.1 leaves the ClusterServiceVersion in a pending state

// 2.1.0:6747

After you upgrade from {product-title-short} version 2.0.4 to version 2.1 and run the `oc get csv` command. In the output, the `PHASE` of your {product-title-short} ClusterServiceVersion (CSV) is `Pending`, but the `NAME` is updated to `advanced-cluster-management.v2.1.0`.

To work around this issue, complete the following steps to find and create the missing `clusterRole` custom resource:

. Find all `clusterrolebinding` resources that were deployed by the {product-title-short} 2.1 CSV by entering the following command: 

+
----
oc get clusterrolebinding |grep advanced-cluster-management
----

+ 
Your output should resemble the following content:
+
----
advanced-cluster-management.v2.1.0-86dfdf7c5d          ClusterRole/advanced-cluster-management.v2.1.0-86dfdf7c5d       9h
advanced-cluster-management.v2.1.0-cd8d57f64           ClusterRole/advanced-cluster-management.v2.1.0-cd8d57f64        9h
----

. Open each `clusterrolebinding` to find the `clusterRole` name that is associated to the `open-cluster-management` service account by entering a command that resembles the following command:
+
----
oc get clusterrolebinding advanced-cluster-management.v2.1.0-cd8d57f64 -o yaml 
----
+
Your output should resemble the following content:
+
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: advanced-cluster-management.v2.1.0-cd8d57f64
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: advanced-cluster-management.v2.1.0-cd8d57f64
subjects:
- kind: ServiceAccount
  name: multicluster-operators
  namespace: open-cluster-management
----

. Manually create any missing `clusterRole` entries by adding content that resembles the following content to your `.yaml` file:
+
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: advanced-cluster-management.v2.1.0-cd8d57f64
rules:
- apiGroups:
  - '*'
  resources:
  - '*'
  verbs:
  - '*'
----

[#installation-known-issues]
== Installation known issues

[#certificate-manager-must-not-exist-during-an-installation]
=== Certificate manager must not exist during an installation

// 1.0.0:678

Certificate manager must not exist on a cluster when you install {product-title}.

When certificate manager already exists on the cluster, {product-title} installation fails.

To resolve this issue, verify if the certificate manager is present in your cluster by running the following command:

----
kubectl get crd | grep certificates.certmanager
----

[#web-console-known-issues]
== Web console known issues

[#search-result-node]
=== Node discrepancy between Cluster page and search results
// 2.0, 2.1, 2.2:9987

You might see a discrepancy between the nodes displayed on the _Cluster_ page and the _Search_ results.

[#ldap-user-names-are-case-sensitive]
=== LDAP user names are case-sensitive

// 1.0.0:before 1.0.0.1

LDAP user names are case-sensitive.
You must use the name exactly the way it is configured in your LDAP directory.

[#console-features-might-not-display-in-firefox-earlier-versions]
=== Console features might not display in Firefox earlier versions

// 1.0.0:before 1.0.0.1

The product supports Mozilla Firefox 74.0 or the latest version that is available for Linux, macOS, and Windows.
Upgrade to the latest version for the best console compatibility.

[#unable-to-search-using-values-with-empty-spaces]
=== Unable to search using values with empty spaces

// 1.0.0:1726

From the console and Visual Web Terminal, users are unable to search for values that contain an empty space.

[#at-logout-user-kubeadmin-gets-extra-browser-tab-with-blank-page]
=== At logout user kubeadmin gets extra browser tab with blank page

// 1.0.0:2191

When you are logged in as `kubeadmin` and you click the *Log out* option in the drop-down menu, the console returns to the login screen, but a browser tab opens with a `/logout` URL.
The page is blank and you can close the tab without impact to your console.

[#secret-content-is-no-longer-displayed]
=== Secret content is no longer displayed

// 2.1.0:6108

For security reasons, search does not display the contents of secrets found on managed clusters. When you search for a secret from the console, you might receive the following error message:

----
Unable to load resource data - Check to make sure the cluster hosting this resource is online
----

[#mco-default-name]
=== Observability not working due to MultiClusterObservability CR name

When you deploy `MultiClusterObservability` custom resource (CR) with a unique name, the metrics data is not collected. The metrics are not collected because the `metrics-collector` is not created. When you deploy observability, {product-title-short} supports using only the default name, `observability`, for the `MultiClusterObservability` CR.

[#cluster-management-issues]
== Cluster management known issues

[#options-missing-bma]
=== New bare metal asset options might not be displayed 
// 2.1.6:11296

After you create and save a bare metal asset, you can select the bare metal asset in the table and apply selected actions. With this issue, you might not see the available actions after selecting a new bare metal asset. Refresh your browser window to restore the actions at the beginning of the table.  

[#no-create-bm-47]
=== Cannot create bare metal managed clusters on {ocp-short} version 4.7
// 2.1.5:10581

You cannot create bare metal managed clusters by using the {product-title-short} hub cluster when the hub cluster is hosted on {ocp-short} version 4.7.

[#create-resource-dropdown-error]
=== Create resource dropdown error
// 2.1:6299 Remove after 2.1.1????

When you detach a managed cluster, the _Create resources_ page might temporarily break and display the following error:

----
Error occurred while retrieving clusters info. Not found.
----

Wait until the namespace automatically gets removed, which takes 5-10 minutes after you detach the cluster. Or, if the namespace is stuck in a terminating state, you need to manually delete the namespace. Return to the page to see if the error resolved.

[#hub-managed-clusters-clock]
=== Hub cluster and managed clusters clock not synced
// 2.1:5636

Hub cluster and manage cluster time might become out-of-sync, displaying in the console `unknown` and eventually `available` within a few minutes. Ensure that the {ocp} hub cluster time is configured correctly. See link:https://docs.openshift.com/container-platform/4.6/installing/install_config/installing-customizing.html[Customizing nodes].

[#console-managed-cluster-inconsistency]
=== Console might report managed cluster policy inconsistency
// 2.0.0:3850

After a cluster is imported, log in to the imported cluster and make sure all pods that are deployed by the {klusterlet} are running. Otherwise, you might see inconsistent data in the console.

For example, if a policy controller is not running, you might not get the same results of violations on the _Governance and risk_ page and the _Cluster status_. 

For instance, you might see 0 violations listed in the _Overview_ status, but you might have 12 violations reported on the _Governance and risk_ page. 

In this case, inconsistency between the pages represents a disconnection between the `policy-controller-addon` on managed clusters and the policy controller on the hub cluster. Additionally, the managed cluster might not have enough resources to run all the {klusterlet} components. 

As a result, the policy was not propagated to managed cluster, or the violation was not reported back from managed clusters.

[#importing-clusters-might-require-two-attempts]
=== Importing clusters might require two attempts

// 2.0.0:3596

When you import a cluster that was previously managed and detached by a {product-title-short} hub cluster, the import process might fail the first time. The cluster status is `pending import`. Run the command again, and the import should be successful. 

[#importing-certain-versions-of-ibm-red-hat-openshift-kubernetes-service-clusters-is-not-supported]
=== Importing certain versions of IBM Red Hat OpenShift Kubernetes Service clusters is not supported

// 1.0.0:2179

You cannot import IBM Red Hat OpenShift Kubernetes Service version 3.11 clusters.
Later versions of IBM OpenShift Kubernetes Service are supported.

[#detaching-openshift-container-platform-3.11-does-not-remove-the-open-cluster-manangement-agent]
=== Detaching {ocp-short} 3.11 does not remove the _open-cluster-management-agent_

// 2.0.0:3847

When you detach managed clusters on {ocp-short} 3.11, the `open-cluster-management-agent` namespace is not automatically deleted. Manually remove the namespace by running the following command:

----
oc delete ns open-cluster-management-agent
----

[#automatic-secret-updates-for-provisioned-clusters-is-not-supported]
=== Automatic secret updates for provisioned clusters is not supported

// 2.0.0:3702

When you change your cloud provider access key, the provisioned cluster access key is not updated in the namespace. This is required when your credentials expire on the cloud provider where the managed cluster is hosted and you try delete the managed cluster. If something like this occurs, run the following command for your cloud provider to update the access key:  

* Amazon Web Services (AWS)

+
----
oc patch secret {CLUSTER-NAME}-aws-creds -n {CLUSTER-NAME} --type json -p='[{"op": "add", "path": "/stringData", "value":{"aws_access_key_id": "{YOUR-NEW-ACCESS-KEY-ID}","aws_secret_access_key":"{YOUR-NEW-aws_secret_access_key}"} }]'
----

* Google Cloud Platform (GCP)

+
You can identify this issue by a repeating log error message that reads, `Invalid JWT Signature` when you attempt to destroy the cluster. If your log contains this message, obtain a new Google Cloud Provider service account JSON key and enter the following command:

+
----
oc set data secret/<CLUSTER-NAME>-gcp-creds -n <CLUSTER-NAME> --from-file=osServiceAccount.json=$HOME/.gcp/osServiceAccount.json
----
+
Replace `CLUSTER-NAME` with the name of your cluster.
+
Replace the path to the file `$HOME/.gcp/osServiceAccount.json` with the path to the file that contains your new Google Cloud Provider service account JSON key. 

* Microsoft Azure 

+
----
oc set data secret/{CLUSTER-NAME}-azure-creds -n {CLUSTER-NAME} --from-file=osServiceAccount.json=$HOME/.azure/osServiceAccount.json
----

* VMware vSphere

+
----
oc patch secret {CLUSTER-NAME}-vsphere-creds -n {CLUSTER-NAME} --type json -p='[{"op": "add", "path": "/stringData", "value":{"username": "{YOUR-NEW-VMware-username}","password":"{YOUR-NEW-VMware-password}"} }]'
----

[#no-run-mgt-ingress-nonroot]
=== Cannot run management ingress as non-root user
//2.0:35532

You must be logged in as `root` to run the `management-ingress` service. 

[#node-information-from-the-managed-cluster-cannot-be-viewed-in-search]
=== Node information from the managed cluster cannot be viewed in search
// 2.0.2:4598

Search maps RBAC for resources in the hub cluster. Depending on user RBAC settings for {product-title-short}, users might not see node data from the managed cluster. Results from search might be different from what is displayed on the _Nodes_ page for a cluster.

[#cluster-might-not-be-destroyed]
=== Process to destroy a cluster does not complete

// 2.1.0:4748

When you destroy a managed cluster, the status continues to display `Destroying` after one hour, and the cluster is not destroyed. To resolve this issue complete the following steps:

. Manually ensure that there are no orphaned resources on your cloud, and that all of the provider resources that are associated with the managed cluster are cleaned up.

. Open the `ClusterDeployment` information for the managed cluster that is being removed by entering the following command:
+
----
oc edit clusterdeployment/<mycluster> -n <namespace>
----
+
Replace _mycluster_ with the name of the managed cluster that you are destroying.
Replace _namespace_ with the namespace of the managed cluster.

. Remove the `hive.openshift.io/deprovision` finalizer to forcefully stop the process that is trying to clean up the cluster resources in the cloud.

. Save your changes and verify that `ClusterDeployment` is gone.

. Manually remove the namespace of the managed cluster by running the following command:
+
----
oc delete ns <namespace>
----
+
Replace _namespace_ with the namespace of the managed cluster.


[#observability-annotation-query-failed]
=== Metrics are unavailable in the Grafana console

* Annotation query failed in the Grafana console: 
// 2.1.0:5625
+
When you search for a specific annotation in the Grafana console, you might receive the following error message due to an expired token: 
+
`"Annotation Query Failed"`
+
Refresh your browser and verify you are logged into your hub cluster.

* Error in _rbac-query-proxy_ pod:
+
Due to unauthorized access to the `managedcluster` resource, you might receive the following error when you query a cluster or project:
+
`no project or cluster found`
+
Check the role permissions and update appropriately. See, link:../security/rbac.adoc#role-based-access-control[Role-based access control] for more information. 

[#application-management-known-issues]
== Application management known issues

[#resource-topology-status-not-deployed]
=== Resource topology status not deployed
// 2.1.0:6106

If your Helm subscription does not have `packageAlias` defined, the resource Topology displays remote cluster resources as `Not deployed`. 

See link:../manage_applications/package_overrides.adoc#configuring-package-overrides[Configuring package overrides] to define the appropriate `packageName` and the `packageAlias`.

[#application-local-cluster-limitation]
=== Application Deploy on local cluster limitation
// 2.1.0:6418

If you select *Deploy on local cluster* when you create or edit an application, the application Topology does not display correctly. *Deploy on local cluster* is the option to deploy resources on your hub cluster so that you can manage it as the `local cluster`, but this is not best practice for this release.

To resolve the issue, see the following procedure:

. Uncheck the *Deploy on local cluster* option in the console.
. Select the *Deploy application resources only on clusters matching specified labels* option.
. Create the following label: `local-cluster : 'true'`

[#edit-application-merge-updates]
=== Merge updates option in the console is unselected when you edit your app
// 2.1.0:6350

In the application console, when you edit your applcation, the *Merge updates* is unselected. You need to select the option again if it was previously selected and you still want to merge your updates.

To verify that merging updates was successful, ensure that the `reconcile-option: merge` is in the YAML subscription annotations.
Complete the following steps in the console:

. Click the `Subscription` node in the resource Topology diagram in the console.
. Click the `View Resource YAML` button in the subscription details pop-up window.
. Verify that the `apps.open-cluster-management.io/reconcile-option: merge` annotation is created on the subscription `.yaml` file.

[#application-private-git-url-limitation]
=== Git branch and URL path fields not populated if a private Git URL exists

// 2.1.0:6045

If you create an application with a _private_ Git repo, and then click *Create application* to create another Git type, the former URL is not populated in the fields in the console. 

The application Editor does not display the channel credential details in this case. If you change the repository authentication information for an existing channel repository, the product cannot manage existing applications that subscribe to that repository.

To resolve this issue, you can update the credential information on the channel resource, or you can delete and recreate the channel.

Use a YAML editor to update the channel resource with the newest credentials. See the sample section of link:../manage_applications/manage_apps_git.adoc##managing-apps-with-git-repositories#[Managing apps with Git repositories].

[#console-pipeline-card-different-data]
=== Console pipeline cards might display different data
// 2.0.0:3703

Search results for your pipeline return an accurate number of resources, but that number might be different in the pipeline card because the card displays resources not yet used by an application.

For instance, after you search for `kind:channel`, you might see you have 10 channels, but the pipeline card on the console might represent only 5 channels that are used.

[#namespace-channel-not-current]
=== Namespace channel 
// 2.0.0:9434

Namespace channel might be functional in code but is currently not a documented option.

[#namespace-channel-subscription-remains-in-failed-state]
=== Namespace channel subscription remains in failed state
// 2.0.0:3581

When you subscribe to a namespace channel and the subscription remains in `FAILED` state after you fixed other associated resources such as channel, secret, configmap, or placement rule, the namespace subscription is not continuously reconciled. 

To force the subscription reconcile again to get out of `FAILED` state, complete the following steps:

. Log in to your hub cluster.
. Manually add a label to the subscription using the following command:

----
oc label subscriptions.apps.open-cluster-management.io the_subscription_name reconcile=true
----

[#deployable-resources-in-a-namespace-channel]
=== Deployable resources in a namespace channel

// 2.0.0:3435

You need to manually create deployable resources within the channel namespace. 

To create deployable resources correctly, add the following two labels that are required in the deployable to the subscription controller that identifies which deployable resources are added:

----
labels:
    apps.open-cluster-management.io/channel: <channel name>
    apps.open-cluster-management.io/channel-type: Namespace
----

Don't specify template namespace in each deployable `spec.template.metadata.namespace`. 

For the namespace type channel and subscription, all the deployable templates are deployed to the subscription namespace on managed clusters. As a result, those deployable templates that are defined outside of the subscription namespace are skipped.

[#edit-role-for-application-error]
=== Edit role for application error

// 2.0.0:1681

A user performing in an `Editor` role should only have `read` or `update` authority on an application, but erroneously editor can also `create` and `delete` an application. Red Hat OpenShift Operator Lifecycle Manager default settings change the setting for the product. To workaround the issue, see the following procedure:

1. Run `oc edit clusterrole applications.app.k8s.io-v1beta1-edit -o yaml` to open the application edit cluster role.
2. Remove `create` and `delete` from the verbs list.
3. Save the change.

[#edit-role-for-placement-rule-error]
=== Edit role for placement rule error

// 2.0.0:3693

A user performing in an `Editor` role should only have `read` or `update` authority on an placement rule, but erroneously editor can also `create` and `delete`, as well. Red Hat OpenShift Operator Lifecycle Manager default settings change the setting for the product. To workaround the issue, see the following procedure:

1. Run `oc edit clusterrole placementrules.apps.open-cluster-management.io-v1-edit` to open the application edit cluster role.
2. Remove `create` and `delete` from the verbs list.
3. Save the change.

[#application-not-deployed-after-an-updated-placement-rule]
=== Application not deployed after an updated placement rule

// 1.0.0:1449

If applications are not deploying after an update to a placement rule, verify that the `klusterlet-addon-appmgr` pod is running.
The `klusterlet-addon-appmgr` is the subscription container that needs to run on endpoint clusters.

You can run `oc get pods -n open-cluster-management-agent-addon` to verify.

You can also search for `kind:pod cluster:yourcluster` in the console and see if the `klusterlet-addon-appmgr` is running.

If you cannot verify, attempt to import the cluster again and verify again.

[#subscription-operator-does-not-create-an-scc]
=== Subscription operator does not create an SCC

// 1.0.0:1764

Learn about {ocp} SCC at https://docs.openshift.com/container-platform/4.3/authentication/managing-security-context-constraints.html#security-context-constraints-about_configuring-internal-oauth[Managing Security Context Constraints (SCC)], which is an additional configuration required on the managed cluster.

Different deployments have different security context and different service accounts.
The subscription operator cannot create an SCC automatically.
Administrators control permissions for pods.
A Security Context Constraints (SCC) CR is required to enable appropriate permissions for the relative service accounts to create pods in the non-default namespace:

To manually create an SCC CR in your namespace, complete the following:

. Find the service account that is defined in the deployments.
For example, see the following `nginx` deployments:
+
----
 nginx-ingress-52edb
 nginx-ingress-52edb-backend
----

. Create an SCC CR in your namespace to assign the required permissions to the service account or accounts.
See the following example where `kind: SecurityContextConstraints` is added:
+
----
 apiVersion: security.openshift.io/v1
 defaultAddCapabilities:
 kind: SecurityContextConstraints
 metadata:
   name: ingress-nginx
   namespace: ns-sub-1
 priority: null
 readOnlyRootFilesystem: false
 requiredDropCapabilities:
 fsGroup:
   type: RunAsAny
 runAsUser:
   type: RunAsAny
 seLinuxContext:
   type: RunAsAny
 users:
 - system:serviceaccount:my-operator:nginx-ingress-52edb
 - system:serviceaccount:my-operator:nginx-ingress-52edb-backend
----

[#application-channels-require-unique-namespaces]
=== Application channels require unique namespaces

// 1.0.0:2311

Creating more than one channel in the same namespace can cause errors with the hub cluster.

For instance, namespace `charts-v1` is used by the installer as a Helm type channel, so do not create any additional channels in `charts-v1`. Ensure that you create your channel in a unique namespace. All channels need an individual namespace, except GitHub channels, which can share a namespace with another GitHub channel.

[#security-known-issues]
== Security known issues

[#internal-error-500-during-login-to-the-console]
=== Internal error 500 during login to the console

// 1.0.1:2414

When {product-title} is installed and the {ocp-short} is customized with a custom ingress certificate, a `500 Internal Error` message appears.
You are unable to access the console because the {ocp-short} certificate is not included in the {product-title} management ingress.
Add the {ocp-short} certificate by completing the following steps:

. Create a ConfigMap that includes the certificate authority used to sign the new certificate. Your ConfigMap must be identical to the one you created in the `openshift-config` namespace. Run the following command:

+
----
oc create configmap custom-ca \
     --from-file=ca-bundle.crt=</path/to/example-ca.crt> \
     -n open-cluster-management
----

. Edit your `multiclusterhub` YAML file by running the following command:

+
----
oc edit multiclusterhub multiclusterhub
----

.. Update the `spec` section by editing the parameter value for `customCAConfigmap`. The parameter might resemble the following content:
+
----
customCAConfigmap: custom-ca
----

After you complete the steps, wait a few minutes for the changes to propagate to the charts and log in again. The {ocp-short} certificate is added.

[#recovering-cert-manager-remove-helm]
=== Recovering _cert-manager_ after removing the helm release

// 2.0.4:5635

If you remove the `cert-manager` and the `cert-manager-webhook-helmreleases`, the Helm releases are triggered to automatically redeploy the charts and generate a new certificate. The new certificate must be synced to the other helm charts that create other {product-title-short} components. To recover the certificate components from the hub cluster, complete the following steps:

. Remove the helm release for `cert-manager` by running the following commands: 
+
----
oc delete helmrelease cert-manager-5ffd5
oc delete helmrelease cert-manager-webhook-5ca82
----

. Verify that the helm release is recreated and the pods are running. 

. Make sure the certificate is generated by running the following command:

+
----
oc get certificates.certmanager.k8s.io
----
+
You might receive the following response:

+
----
(base) ➜  cert-manager git:(master) ✗ oc get certificates.certmanager.k8s.io
NAME                                            READY   SECRET                                          AGE   EXPIRATION
multicloud-ca-cert                              True    multicloud-ca-cert                              61m   2025-09-27T17:10:47Z
----

. Update the other components with this certificate, by downloading and running https://gist.github.com/cdoan1/79451b3d75a2f3f1b74b3622029d6989[`generate-update-issuer-cert-manifest.sh` script].

. Verify that all of the secrets from `oc get certificates.certmanager.k8s.io` have the ready state `True`.  
